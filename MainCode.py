# -*- coding: utf-8 -*-
"""Hybrid GRU-CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RFqz3gaiqz3TQD0kge5BKJ6j78t3b6_u
"""

import pandas as pd
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout, Flatten
from tensorflow.keras.utils import to_categorical

# Uploading CSV DataSet
file_path = '/content/balanced_dataset.csv'  # Update with the actual filename if necessary
df = pd.read_csv(file_path)

# Checking if data has been loaded accurately by loading first few rows
df.head()

# Checking for missing values
df.isnull().sum()

# Checking column names
df.columns

# Removing leading and trailing spaces from column names
df.columns = df.columns.str.strip()

# Filtering the dataset based on the 'Label' column
benign_data = df[df['Label'] == 'Benign']
ddos_data = df[df['Label'] == 'DDoS']

# Printing the class distribution to ensure we have the correct splits
print("Class Distribution in Dataset:")
print(df['Label'].value_counts())

# Checking for missing values
missing_values = X.isnull().sum()

# Checking for infinite values
infinite_values = (X == float('inf')).sum() + (X == -float('inf')).sum()

print("Missing values in each column:\n", missing_values)
print("Total number of infinite values:", infinite_values)

# Checking for the maximum and minimum values of the features
print(X.describe())

# Checking for infinity or extremely large values in X_train and X_test
import numpy as np

# Checking if there are any infinite values in the dataset
print("Any Infinite values in X_train: ", np.isinf(X_train).sum())
print("Any Infinite values in X_test: ", np.isinf(X_test).sum())

# Replacing infinite values with a large number or NaN (for later removal)
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

# Checking if there are any NaN values now
print("Any NaN values in X_train: ", X_train.isna().sum())
print("Any NaN values in X_test: ", X_test.isna().sum())

# Replacing NaN values with the median of each column (or another strategy)
X_train.fillna(X_train.median(), inplace=True)
X_test.fillna(X_test.median(), inplace=True)

# Scaling again
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Checking the maximum values in the dataset
print("Maximum values in X_train:\n", X_train.max())
print("Maximum values in X_test:\n", X_test.max())

# Hybrid model core

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, GRU, Dense, Dropout

model = Sequential()

# CNN Part
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(sequence_length, num_features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))

# Flatten before passing to GRU
model.add(Flatten())

# GRU Part
model.add(GRU(64, return_sequences=True))
model.add(GRU(32))

# Dense Layers for final output
model.add(Dense(1, activation='sigmoid'))

# Capping the values at the 99th percentile of each feature
max_vals = X_train.quantile(0.99, axis=0)

# Clipping the values in both X_train and X_test column-wise
X_train = X_train.apply(lambda col: col.clip(upper=max_vals[col.name]), axis=0)
X_test = X_test.apply(lambda col: col.clip(upper=max_vals[col.name]), axis=0)

# Scaling the data
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Adding CNN layers

model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

# Adding GRU layer

model.add(GRU(units=64, return_sequences=False))
model.add(Dropout(0.3))

# Dense layers for final prediction

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(y.shape[1], activation='softmax'))  # 'softmax' for multi-class classification

# Step 4: Compile the Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
rf_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test_scaled)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
